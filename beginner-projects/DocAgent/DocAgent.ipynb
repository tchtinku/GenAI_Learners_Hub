{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFKMKaZfJSms"
      },
      "outputs": [],
      "source": [
        "!pip install -q pdfplumber sentence-transformers gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "import numpy as np\n",
        "import tempfile\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gradio as gr\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "OAv8Y7lSJV3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "F1j_yd2JJX0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define task-related embeddings\n",
        "task_keywords = ['review', 'complete', 'submit', 'schedule', 'finalize',\n",
        "                 'prepare', 'organize', 'plan', 'assign', 'follow up',\n",
        "                 'update', 'approve', 'action item', 'next step']\n",
        "task_embeddings = model.encode(task_keywords)"
      ],
      "metadata": {
        "id": "gGJ9YjSFJaSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file_path):\n",
        "    \"\"\"Improved text extraction with error handling\"\"\"\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            return \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting text: {str(e)}\""
      ],
      "metadata": {
        "id": "nnga6nfJJcpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_task_sentence(sentence, threshold=0.6):\n",
        "    \"\"\"Use semantic similarity to detect tasks\"\"\"\n",
        "    sentence_embedding = model.encode([sentence])\n",
        "    similarities = cosine_similarity(sentence_embedding, task_embeddings)\n",
        "    return np.max(similarities) > threshold"
      ],
      "metadata": {
        "id": "S1TAtH6tJrZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tasks_from_text(text):\n",
        "    \"\"\"Improved task extraction with semantic analysis\"\"\"\n",
        "    tasks = []\n",
        "    # Safer sentence splitting\n",
        "    sentences = re.split(r'(?<=[\\.\\?\\!])\\s+', text)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if is_task_sentence(sentence):\n",
        "            cleaned = re.sub(r'\\s+', ' ', sentence.strip())\n",
        "            cleaned = re.sub(r'\\[[^\\]]*\\]', '', cleaned)  # Remove [HIGH] etc.\n",
        "            tasks.append(cleaned)\n",
        "\n",
        "    return tasks[:10]"
      ],
      "metadata": {
        "id": "Cwak4dFQJhTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(file):\n",
        "    import os, tempfile\n",
        "    try:\n",
        "        if file is None:\n",
        "            return {\"error\": \"No file uploaded\"}\n",
        "\n",
        "        if hasattr(file, \"read\"):\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
        "                tmp.write(file.read())\n",
        "                tmp_path = tmp.name\n",
        "        elif isinstance(file, str) and os.path.exists(file):\n",
        "            tmp_path = file\n",
        "        else:\n",
        "            return {\"error\": \"Invalid file input\"}\n",
        "\n",
        "        text = extract_text_from_pdf(tmp_path)\n",
        "        if text.startswith(\"Error\"):\n",
        "            return {\"error\": text}\n",
        "\n",
        "        # \ud83d\udc47 Check what was extracted\n",
        "        return {\"extracted_text\": text}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Processing failed: {str(e)}\"}"
      ],
      "metadata": {
        "id": "_S1TLL2hJjR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio UI\n",
        "iface = gr.Interface(\n",
        "    fn=process_document,\n",
        "    inputs=gr.File(label=\"\ud83d\udcc4 Upload PDF Document\", file_types=['.pdf']),\n",
        "    outputs=gr.JSON(label=\"\ud83d\ude80 Extracted Tasks\"),\n",
        "    title=\"DocAgent - AI Workflow Automator\",\n",
        "    description=\"Upload a PDF to extract actionable tasks using AI\",\n",
        "    examples=[\n",
        "        [\"./meeting_notes.pdf\"],\n",
        "        [\"./project_plan.pdf\"]\n",
        "    ],\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "id": "hoTsDiztJxBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}